{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAC-2 Code And Details\n",
    "This notebook contains all code and explanations of said code. Code is separated into blocks, organised and split by purpose of code. \n",
    "\n",
    "The method used for this assessment to evolve agents throughout gameplay was a neural network, where weights of said neural network are evolved generation by generation. Further detail can be found at the relevant code sections within this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Representation\n",
    "As each agent is required to be part of a group, groups were defined as an integer with representation as follows:\n",
    "* `0` - Saints\n",
    "* `1` - Buddies\n",
    "* `2` - Fight Club\n",
    "* `3` - Vandals\n",
    "\n",
    "An agent was decided to be represented using a class, this class could then be used to keep track of all important agent attributes:\n",
    "* `wealth` - The total wealth of the agent.\n",
    "* `startingGroup` - The initial group assignment of the agent, reassigned each generation start. This is randomly allocated when the agent is first created.\n",
    "* `group` - The current group assignment of the agent.\n",
    "* `gameCount` - The number of games played by the agent.\n",
    "* `weights` - List of floats used in the neural network to make a decision on which group to join. Evolved by the evolutionary algorithm.\n",
    "* `fitness` - Current fitness of the agent. \n",
    "\n",
    "The game can be played by calling the `getPayoff()` method with the opponent as the required parameter. This is called for each of the two agents chosen in a single game as each agent assumes the role of opponent to the other. Representing each group as an integer allows for very simple calculation of payoffs, with a lookup table implemented for each agent as follows: `[[4,0,4,0],[6,4,6,1],[4,0,1,0],[6,1,6,0]]`. This lookup table stores the payoffs for each group interaction with each group, for a total of 16 different possible interactions. \n",
    "\n",
    "The evolutionary aspect of the player class is called with `evaluate()`, with the required parameters of the current game opponent along with an instance of the neural network class. This method loads the agent weights into the neural network, calculates the group that the agent should be assigned to (this is the agent deciding if it should move group) and then the agent fitness is assigned as the agent wealth divided by the game count. This fitness was chosen to allow for a fair comparison between agents regardless of how many games they were randomly chosen to play.\n",
    "\n",
    "The `reset()` method is called to setup an agent ready for a new generation. This resets the group to the agents starting group, along with setting the agent wealth, fitness, and game counter to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class player():\n",
    "  def __init__(self, IND_SIZE):\n",
    "    self.wealth = 0\n",
    "    self.startingGroup = random.randint(0,3)\n",
    "    self.group = self.startingGroup\n",
    "    self.gameCount = 0\n",
    "    self.weights = tools.initRepeat(list, toolbox.attr_float, IND_SIZE)\n",
    "    self.fitness = 0\n",
    "  \n",
    "  def evaluate(self,opponent,network):\n",
    "    network.setWeightsLinear(self.weights)\n",
    "    output = network.feedForward([self.wealth, self.group, opponent.wealth, opponent.group])\n",
    "    decision = np.argmax(output, axis=0)\n",
    "    if (self.group != decision):\n",
    "      self.group = decision\n",
    "    self.fitness = self.wealth / self.gameCount\n",
    "\n",
    "  def addPayoff(self, opponent):\n",
    "    payoffs = [[4,0,4,0],[6,4,6,1],[4,0,1,0],[6,1,6,0]]\n",
    "    self.wealth += payoffs[self.group][opponent.group]\n",
    "    self.gameCount += 1\n",
    "\n",
    "  def reset(self):\n",
    "    self.wealth = 0\n",
    "    self.gameCount = 0\n",
    "    self.group = self.startingGroup\n",
    "    self.fitness = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evolution Environment\n",
    "\n",
    "### Neural Network\n",
    "The neural network used by all agents in the system is a single hidden layer network with 4 input nodes, 8 hidden nodes and 4 output nodes. These 4 output nodes represent the decision of which group to join, and a group is selected using a `softmax` function. The input nodes of this neural network are the agents own group, the agents own wealth, the opponents group and the opponents wealth. These inputs were settled on after experimentation with other inputs such as the number of games the agent had played, or the current game number. These additional inputs did not improve performance and were removed to reduce chance of overfitting.\n",
    "\n",
    "A bias of 1 was added to the input layer to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numInputNodes = 4\n",
    "numHiddenNodes = 8\n",
    "numOutputNodes = 4\n",
    "\n",
    "IND_SIZE = ((numInputNodes+1) * numHiddenNodes) +  + (numHiddenNodes * numOutputNodes)\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "  def __init__(self, numInput, numHidden, numOutput):\n",
    "    self.numInput = numInput + 1\n",
    "    self.numHidden = numHidden\n",
    "    self.numOutput = numOutput\n",
    "\n",
    "    self.wh = np.random.randn(self.numHidden, self.numInput) \n",
    "    self.wo = np.random.randn(self.numOutput, self.numHidden)\n",
    "\n",
    "    self.ReLU = lambda x : max(0,x)\n",
    "\n",
    "  def softmax(self, x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "  def feedForward(self, inputs):\n",
    "    inputsBias = inputs[:]\n",
    "    inputsBias.insert(len(inputs), 1)\n",
    "\n",
    "    h1 = np.dot(self.wh, inputsBias)\n",
    "    h1 = [self.ReLU(x) for x in h1]\n",
    "\n",
    "    output = np.dot(self.wo, h1)\n",
    "    return self.softmax(output)\n",
    "\n",
    "  def getWeightsLinear(self):\n",
    "    flat_wh = list(self.wh.flatten())\n",
    "    flat_wo = list(self.wo.flatten())\n",
    "    return( flat_wh + flat_wo )\n",
    "\n",
    "  def setWeightsLinear(self, Wgenome):\n",
    "    numWeights_IH = self.numHidden * (self.numInput)\n",
    "    self.wh = np.array(Wgenome[:numWeights_IH])\n",
    "    self.wh = self.wh.reshape((self.numHidden, self.numInput))\n",
    "    self.wo = np.array(Wgenome[numWeights_IH:])\n",
    "    self.wo = self.wo.reshape((self.numOutput, self.numHidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Simulation\n",
    "The game simulation is defined as: for a set number of games, two agents are selected at random from a population to play against each other. Payoffs are calculated and each agent is then given the opportunity to change groups. \n",
    "\n",
    "This is implemented within the method `playGameAndEvolve` using the constant `NGAMES` to define the number of games played, and `POP` to specify the population size. The two agents are selected using the code `random.sample(range(POP),2)`, which selects two distinct population indexes, these agents then calculate their respective payoffs using `addPayoff`, and calculate their fitness and decision on if they should migrate groups using `evaluate`. Details of these methods are found in the Agent Representation part of this code.\n",
    "\n",
    "`playBasicGame` is an additional method that can be used to run the game without allowing agents to swap their group assignments, this will be used to evaluate the behaviour developed through adaption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGAMES = 10000\n",
    "POP = 4*400\n",
    "\n",
    "def playGameAndEvolve(pop, network):\n",
    "  for r in range(NGAMES):\n",
    "    selection = random.sample(range(POP),2)\n",
    "    pop[selection[0]].addPayoff(pop[selection[1]])\n",
    "    pop[selection[1]].addPayoff(pop[selection[0]])\n",
    "    pop[selection[0]].evaluate(pop[selection[1]],network)\n",
    "    pop[selection[1]].evaluate(pop[selection[0]],network)\n",
    "  return pop\n",
    "\n",
    "def playBasicGame(pop):\n",
    "  for r in range(NGAMES):\n",
    "    selection = random.sample(range(POP),2)\n",
    "    pop[selection[0]].addPayoff(pop[selection[1]])\n",
    "    pop[selection[1]].addPayoff(pop[selection[0]])\n",
    "  return pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Adaptation Procedure (Training through Evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumGroups(groups):\n",
    "  groupTotals=[0,0,0,0]\n",
    "  for person in groups:\n",
    "    groupTotals[person.startingGroup] += person.wealth\n",
    "  return groupTotals\n",
    "\n",
    "def avgGroups(groups):\n",
    "  groupTotals=[0,0,0,0]\n",
    "  counts=[0,0,0,0]\n",
    "  for person in groups:\n",
    "    if (person.gameCount > 0):\n",
    "      groupTotals[person.startingGroup] += person.wealth\n",
    "      counts[person.startingGroup] += 1\n",
    "  for i in range(0,4):\n",
    "    if(groupTotals[i] > 0):\n",
    "      groupTotals[i] = groupTotals[i] / counts[i] \n",
    "  return groupTotals\n",
    "\n",
    "def countGroups(pop):\n",
    "  groupTotals=[0,0,0,0]\n",
    "  for person in pop:\n",
    "    groupTotals[person.group] += 1\n",
    "  return groupTotals\n",
    "\n",
    "def fitnessStats(pop):\n",
    "  fitnessSum = 0\n",
    "  fitnessMax = 0 \n",
    "  fitnessMin = -1\n",
    "  for person in pop:\n",
    "    if(person.fitness > fitnessMax):\n",
    "      fitnessMax = person.fitness\n",
    "    if(person.fitness < fitnessMin or fitnessMin == -1):\n",
    "      fitnessMin = person.fitness\n",
    "    fitnessSum += person.fitness\n",
    "  return { \"mean\": fitnessSum/len(pop), \"max\":fitnessMax, \"min\":fitnessMin }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generation 0 --\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-1dd675b65e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayGameAndEvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffspring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Group avg    : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgGroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Group sum    : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumGroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-d810e0bc6ceb>\u001b[0m in \u001b[0;36mplayGameAndEvolve\u001b[0;34m(pop, network)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddPayoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-7d4ee19a1050>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, opponent, network)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetWeightsLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwealth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwealth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-6627ba4336d7>\u001b[0m in \u001b[0;36msetWeightsLinear\u001b[0;34m(self, Wgenome)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumHidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWgenome\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumWeights_IH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumHidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from deap import base\n",
    "from deap import tools\n",
    "import random\n",
    "\n",
    "INDPB=0.1\n",
    "NGEN = 50\n",
    "CXPB = 0.1\n",
    "MUTPB = 0.5\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", random.uniform, -1.0, 1.0)\n",
    "toolbox.register(\"individual\", player, IND_SIZE)\n",
    "toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0.0, sigma=0.5, indpb=INDPB)\n",
    "toolbox.register(\"pop\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "network = NeuralNetwork(numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "\n",
    "pop = toolbox.pop(n=POP)\n",
    "for g in range(NGEN):\n",
    "  print(\"-- Generation %i --\" % g)\n",
    "  offspring = toolbox.select(pop, len(pop))\n",
    "\n",
    "  offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "  for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "    if random.random() < CXPB:\n",
    "      toolbox.mate(child1.weights, child2.weights)\n",
    "\n",
    "  for mutant in offspring:\n",
    "    if random.random() < MUTPB:\n",
    "      toolbox.mutate(mutant.weights)\n",
    "      if (random.random() < INDPB):\n",
    "        mutant.startingGroup = random.randint(0,3)\n",
    "\n",
    "  for person in offspring:\n",
    "    person.reset()\n",
    "  \n",
    "  pop[:] = playGameAndEvolve(offspring, network)\n",
    "  print(\"Group avg    : \" + str(avgGroups(pop)))\n",
    "  print(\"Group sum    : \" + str(sumGroups(pop)))\n",
    "  print(\"Group count  : \"+str(countGroups(pop)))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
